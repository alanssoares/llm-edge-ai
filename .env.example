# LLM Edge AI - Environment Variables Example
# Copy this file to .env and customize as needed

# ===== Device Configuration =====
DEVICE_NAME=edge-device-01
DEVICE_ID=00:0f:00:70:91:0a

# ===== MQTT Configuration =====
MQTT_BROKER=mqtt-broker
MQTT_PORT=1883

# ===== Logging Configuration =====
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# ===== Dataset Configuration =====
DATASET_PATH=/app/dataset/iot_telemetry_data.csv

# ===== LLM Configuration =====

# Enable or disable LLM inference
ENABLE_LLM=false

# Model selection - change this to compare different models
# Recommended models:
#   microsoft/Phi-3.5-mini-instruct (default, ~7B params, balanced)
#   TinyLlama/TinyLlama-1.1B-Chat-v1.0 (lightweight, fast)
#   microsoft/Phi-3-mini-4k-instruct (compact variant)
#   google/gemma-2b-it (Google's efficient model)
#   stabilityai/stablelm-2-1_6b (Stability AI)
LLM_MODEL_NAME=microsoft/Phi-3.5-mini-instruct

# Run LLM inference every N telemetry messages
# Higher values = less frequent inference, lower resource usage
# Lower values = more frequent inference, more metrics data
LLM_INFERENCE_INTERVAL=5

# Maximum token length for LLM generation
# Lower values = faster inference, shorter responses
# Higher values = slower inference, longer responses
LLM_MAX_LENGTH=512

# Sampling temperature (0.0 - 1.0)
# Lower values = more deterministic, focused
# Higher values = more creative, diverse
LLM_TEMPERATURE=0.7

# ===== Performance Tuning Examples =====

# For resource-constrained devices:
# ENABLE_LLM=true
# LLM_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
# LLM_INFERENCE_INTERVAL=10
# LLM_MAX_LENGTH=256
# LLM_TEMPERATURE=0.3

# For high-performance devices:
# ENABLE_LLM=true
# LLM_MODEL_NAME=microsoft/Phi-3.5-mini-instruct
# LLM_INFERENCE_INTERVAL=3
# LLM_MAX_LENGTH=1024
# LLM_TEMPERATURE=0.7

# ===== Notes =====
# - First run will download the model (~2-7GB depending on model)
# - Larger models require more memory and are slower
# - Metrics are automatically saved to ./metrics/{device_name}/
# - Energy consumption is estimated based on CPU usage
# - GPU will be used automatically if available (CUDA)
